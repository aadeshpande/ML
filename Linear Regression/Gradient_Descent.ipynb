{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/narayani/Desktop/IIITB/Sem6/ML/ML_Project/Housing_Linear Regression/data/housing.csv')#Plese give the complete path of the data file\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def splitTrainData(total_data, ratio):\n",
    "    indices_Shuffled = np.random.permutation(len(total_data))\n",
    "    test_data_size = int(len(total_data)* ratio)\n",
    "    indices_Test = indices_Shuffled[:test_data_size]\n",
    "    indices_Train = indices_Shuffled[test_data_size:]\n",
    "    return total_data.iloc[indices_Train], total_data.iloc[indices_Test]\n",
    "training_data, test_data = splitTrainData(data, 0.3)'''\n",
    "\n",
    "#Splitting the data into training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_set, test_set = train_test_split(data,test_size=0.2,random_state = 42)\n",
    "#print(\"Train: \",len(train_set), \" +  Test: \" , len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " #X_training = training_data.drop(['median_house_value'],axis =1)\n",
    "# Y_labels = training_data[\"median_house_value\"].copy()\n",
    "\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# imputer = Imputer(strategy = \"median\")\n",
    "\n",
    "# imputer.fit(X_training)\n",
    "# #imputer.statistics_\n",
    "# #X_training.median().values\n",
    "\n",
    "# X_imp = imputer.transform(X_training)\n",
    "\n",
    "# X_inp = pd.DataFrame(X_imp, columns = X_training.columns)\n",
    "\n",
    "# # X_inp['ocean_new']=data.apply(changing_df,axis = 1)\n",
    "# # X_inp['ocean_1h_ocean']=data.apply(changing_ocean_1h_ocean,axis = 1)\n",
    "# # X_inp['ocean_inland']=data.apply(changing_ocean_inland,axis = 1)\n",
    "# # X_inp['ocean_near_ocean']=data.apply(changing_ocean_near_ocean,axis = 1)\n",
    "# # X_inp['ocean_near_bay']=data.apply(changing_ocean_near_bay,axis = 1)\n",
    "# # X_inp['ocean_island']=data.apply(changing_ocean_island,axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_data = X_inp.as_matrix()\n",
    "# Y = Y_labels.as_matrix()\n",
    "\n",
    "# X = np.c_[np.ones((len(X_inp),1)),X_data]\n",
    "# #print (X)\n",
    "\n",
    "# X= (X - X.mean())/(X+X.std())\n",
    "# #print X\n",
    "# m = len(X)\n",
    "\n",
    "#theta = np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process the data:\n",
    "                To remove \"NAN-Values\" wherever there in the data. And Since, column named \"Ocean Proximity\" is a text field, we have encode it, so that it can be used for ML-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData ( train_set ):\n",
    "#Removing the target values and copying them as labels/target value for feature values of each data-point.\n",
    "#new data is the data without the target values\n",
    "    new_data = train_set.drop(\"median_house_value\", axis = 1)\n",
    "    labels = train_set[\"median_house_value\"].copy()\n",
    "    \n",
    "#Imputer is used to remove all the NAN values in the dataset, but after the removal of the text field i.e \"ocean_proximity\"\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imputer = Imputer(strategy = \"median\")  \n",
    "    ntx_data = new_data.drop(\"ocean_proximity\", axis =1)\n",
    "    imputer.fit(ntx_data)\n",
    "    ntx_data_imp = imputer.transform(ntx_data)\n",
    "    \n",
    "#Converting into Pandas-Dataframe. \n",
    "    ntx_data_inp = pd.DataFrame(ntx_data_imp, columns = ntx_data.columns,index = list(ntx_data.index.values))\n",
    "    \n",
    "#Encoding the text-field \"ocean_proximity\".\n",
    "    housing_cat = new_data['ocean_proximity']\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "    encoder = OneHotEncoder()\n",
    "#Reshaping housing_cat_encoder as it is 1-D array, but fit_transform expects a 2-D array.\n",
    "    housing_cat_1hot =encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "    (housing_cat_1hot.toarray())\n",
    "\n",
    "    import category_encoders as c_enc\n",
    "    encoder = c_enc.OneHotEncoder()\n",
    "    housing_cat_reshaped = housing_cat.values.reshape(-1,1)\n",
    "    encoder.fit(housing_cat_reshaped)\n",
    "    \n",
    "    X_cleaned = encoder.transform(housing_cat_reshaped)\n",
    "#Converting Encoded values for \"ocean_proximity\" to matrix\n",
    "    enc_data = X_cleaned.as_matrix()\n",
    "    \n",
    "#Feature Scaling for data without the encoded values for \"ocean_proximity\":\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(ntx_data_inp)\n",
    "    data_without_enc = scaler.transform(ntx_data_inp)\n",
    "\n",
    "#data without encoded values is being appended by the encoded values to get the complete data for computation.\n",
    "    data_training = np.append(data_without_enc, enc_data, axis = 1)\n",
    "    data_training = X = np.c_[np.ones((len(data_training),1)),data_training]\n",
    "    \n",
    "    \n",
    "    return data_training, labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data , training_labels = preprocessData(training_set)\n",
    "testing_data , testing_labels = preprocessData(test_set)\n",
    "housing_data , housing_labels = preprocessData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eta = 0.01  # Learning-Rate\n",
    "n_iteration = 1500 # Number of times we are going to iterate to get the local minimum\n",
    "\n",
    "m = len(training_data) # Number of Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing the parameter matrix(theta) to a zero matrix\n",
    "theta = [0.0]*training_data.shape[1]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Gradient:\n",
    "def calc_gradient(input_matrix, parameter_matrix, actual_labels, numberofInputs):\n",
    "    hypothesis = input_matrix.dot(theta)\n",
    "    #print temp_X\n",
    "    err = hypothesis - actual_labels\n",
    "    #X_T = input_matrix.T\n",
    "    temp = input_matrix.T.dot(err)\n",
    "    #print temp\n",
    "    return (2.0/numberofInputs)*(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "        Here we update the parameter matrix at every step and move towards a local minimum to fit the data to a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5521531105\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "t1= timeit.default_timer()\n",
    "for i in range(n_iteration):\n",
    "    gradient = calc_gradient(training_data,theta,training_labels,m)\n",
    "    #hypothesis = X.dot(theta)\n",
    "#   print hypothesis\n",
    "    #err = hypothesis - y\n",
    "#   print err\n",
    "    #gradient = ((X.T.dot(err)))*(2.0/m)\n",
    "#   print \"Gradient: \"\n",
    "    #print gradient\n",
    "    #gradient = (2/m)* X.T.dot(X.dot(theta) - Y)\n",
    "    theta = theta - eta*gradient\n",
    "t2= timeit.default_timer()-t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted labels for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193094.82827302 292944.21446885 243956.31969276 ... 193475.23214682\n",
      " 281563.83383347 266448.93574372]\n"
     ]
    }
   ],
   "source": [
    "train_predicted_Y = training_data.dot(theta.T)\n",
    "print train_predicted_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_data.drop(['median_house_value','ocean_proximity','ocean_new'], axis =1)\n",
    "# Y_actuals = test_data[\"median_house_value\"].copy()\n",
    "\n",
    "# imputer.fit(X_test)\n",
    "# X_test_imp = imputer.transform(X_test)\n",
    "# test_inp = pd.DataFrame(X_test_imp, columns = X_test.columns)\n",
    "\n",
    "# data_test = test_inp.as_matrix()\n",
    "# actual_Y = Y_actuals.as_matrix()\n",
    "\n",
    "\n",
    "# #test_inp.info()\n",
    "# actual_Y\n",
    "# #test = np.c_[np.ones((len(test_inp)),1),data_test]\n",
    "# test = np.c_[np.ones((len(test_inp),1)),data_test]\n",
    "# #test\n",
    "\n",
    "# test = (test - test.mean())/(test+test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicated labels for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109540.7876531 , 177509.82704962, 224560.24031249, ...,\n",
       "       442858.01861914, 180474.74849168, 185651.93264162])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_predicted_Y = testing_data.dot(theta.T)\n",
    "\n",
    "test_predicted_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_housing = data.drop(['median_house_value', 'ocean_proximity','ocean_new'],axis =1)\n",
    "# Y_housing = data[\"median_house_value\"].copy()\n",
    "\n",
    "# imputer.fit(X_housing)\n",
    "\n",
    "# X_housing_imp = imputer.transform(X_housing)\n",
    "# X_housing_inp = pd.DataFrame(X_housing_imp, columns = X_housing.columns)\n",
    "\n",
    "# X_housing_data = X_housing_inp.as_matrix()\n",
    "# housing_Y = Y_housing.as_matrix()\n",
    "# housing = np.c_[np.ones((len(X_housing_inp),1)),X_housing_data]\n",
    "# housing = (housing - housing.mean())/(housing + housing.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicated labels for housing dataset without splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([421012.83273466, 435626.65596248, 391760.95741584, ...,\n",
       "        96384.81031016, 106444.89293156, 123277.11750073])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predicted_Y = housing_data.dot(theta.T)\n",
    "housing_predicted_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Traning Data: 68615.9178713624\n",
      "RMSE for Test Data: 78336.17837219569\n",
      "RMSE for Housing Data taken without splitting: 81214.52150474883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_mse = mean_squared_error(testing_labels, test_predicted_Y)\n",
    "train_mse = mean_squared_error(training_labels,train_predicted_Y)\n",
    "housing_mse = mean_squared_error(housing_labels, housing_predicted_Y)\n",
    "\n",
    "print \"RMSE for Traning Data: \"+ str(np.sqrt(train_mse))\n",
    "print \"RMSE for Test Data: \" + str(np.sqrt(test_mse))\n",
    "print \"RMSE for Housing Data taken without splitting: \"+ str(np.sqrt(housing_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
