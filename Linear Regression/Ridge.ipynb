{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib as hl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/narayani/Desktop/IIITB/Sem6/ML/ML_Project/Housing_Linear Regression/data/housing.csv')\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def splitTrainData(total_data, ratio):\n",
    "    indices_Shuffled = np.random.permutation(len(total_data))\n",
    "    test_data_size = int(len(total_data)* ratio)\n",
    "    indices_Test = indices_Shuffled[:test_data_size]\n",
    "    indices_Train = indices_Shuffled[test_data_size:]\n",
    "    return total_data.iloc[indices_Train], total_data.iloc[indices_Test]\n",
    "training_data, test_data = splitTrainData(data, 0.3)'''\n",
    "\n",
    "#Splitting the whole data into training set and test set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_set, test_set = train_test_split(data,test_size=0.2,random_state = 42)\n",
    "#print(\"Train: \",len(train_set), \" +  Test: \" , len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_training = training_data.drop(['median_house_value', 'ocean_proximity'],axis =1)\n",
    "# Y_labels = training_data[\"median_house_value\"].copy()\n",
    "\n",
    "# from sklearn.preprocessing import Imputer\n",
    "# imputer = Imputer(strategy = \"median\")\n",
    "\n",
    "# imputer.fit(X_training)\n",
    "# #imputer.statistics_\n",
    "# #X_training.median().values\n",
    "\n",
    "# X_imp = imputer.transform(X_training)\n",
    "# X_inp = pd.DataFrame(X_imp, columns = X_training.columns)\n",
    "\n",
    "# X_data = X_inp.as_matrix()\n",
    "# Y = Y_labels.as_matrix()\n",
    "\n",
    "# X = np.c_[np.ones((len(X_inp),1)),X_data]\n",
    "# #X= (X - X.mean())/(X+X.std())\n",
    "# theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "\n",
    "# theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process the data:\n",
    "                To remove \"NAN-Values\" wherever there in the data. And Since, column named \"Ocean Proximity\" is a text field, we have encode it, so that it can be used for ML-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData ( train_set ):\n",
    "#Removing the target values and copying them as labels/target value for feature values of each data-point.\n",
    "#new data is the data without the target values\n",
    "    new_data = train_set.drop(\"median_house_value\", axis = 1)\n",
    "    labels = train_set[\"median_house_value\"].copy()\n",
    "    \n",
    "#Imputer is used to remove all the NAN values in the dataset, but after the removal of the text field i.e \"ocean_proximity\"\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imputer = Imputer(strategy = \"median\")  \n",
    "    ntx_data = new_data.drop(\"ocean_proximity\", axis =1)\n",
    "    imputer.fit(ntx_data)\n",
    "    ntx_data_imp = imputer.transform(ntx_data)\n",
    "    \n",
    "#Converting into Pandas-Dataframe. \n",
    "    ntx_data_inp = pd.DataFrame(ntx_data_imp, columns = ntx_data.columns,index = list(ntx_data.index.values))\n",
    "    \n",
    "#Encoding the text-field \"ocean_proximity\".\n",
    "    housing_cat = new_data['ocean_proximity']\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "    encoder = OneHotEncoder()\n",
    "#Reshaping housing_cat_encoder as it is 1-D array, but fit_transform expects a 2-D array.\n",
    "    housing_cat_1hot =encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "    (housing_cat_1hot.toarray())\n",
    "\n",
    "    import category_encoders as c_enc\n",
    "    encoder = c_enc.OneHotEncoder()\n",
    "    housing_cat_reshaped = housing_cat.values.reshape(-1,1)\n",
    "    encoder.fit(housing_cat_reshaped)\n",
    "    \n",
    "    X_cleaned = encoder.transform(housing_cat_reshaped)\n",
    "#Converting Encoded values for \"ocean_proximity\" to matrix\n",
    "    enc_data = X_cleaned.as_matrix()\n",
    "    \n",
    "#Feature Scaling for data without the encoded values for \"ocean_proximity\":\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(ntx_data_inp)\n",
    "    data_without_enc = scaler.transform(ntx_data_inp)\n",
    "\n",
    "#data without encoded values is being appended by the encoded values to get the complete data for computation.\n",
    "    data_training = np.append(data_without_enc, enc_data, axis = 1)\n",
    "    data_training = X = np.c_[np.ones((len(data_training),1)),data_training]\n",
    "    \n",
    "    \n",
    "    return data_training, labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the Training Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data , training_labels = preprocessData(training_set)\n",
    "testing_data , testing_labels = preprocessData(test_set)\n",
    "#Hyperparameter\n",
    "alpha = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = training_data.shape[1]\n",
    "I = np.identity(n)\n",
    "I[0,0] = 0\n",
    "#Matrix in which Identity matrix first element is replaced by 0 instead of 1.\n",
    "A = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(training_data, training_labels)\n",
    "# lin_reg.predict(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the parameter matrix from Ridge Equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([238252.92341126, -53822.57959311, -54412.58749094,  13890.41026217,\n",
       "       -13090.60576714,  43065.99923065, -43402.47060121,  18379.81171344,\n",
       "        75166.34517979, -14920.2688228 , -58140.53857764, -18352.46936135,\n",
       "       -23486.15567601, 114899.43244337,      0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temp = (training_data.T).dottraining_data)\n",
    "#Calculating Parameters with Ridge Equation.\n",
    "theta = np.linalg.inv(training_data.T.dot(training_data) + (alpha)*A).dot(training_data.T).dot(training_labels)\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted labels for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([188631.70020122, 290380.97260532, 250983.12775868, ...,\n",
       "       194624.89800017, 281816.41515812, 271570.77765465])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted_Y = training_data.dot(theta.T)\n",
    "train_predicted_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_data.drop(['median_house_value','ocean_proximity'], axis =1)\n",
    "# Y_actuals = test_data[\"median_house_value\"].copy()\n",
    "\n",
    "# imputer.fit(X_test)\n",
    "# X_test_imp = imputer.transform(X_test)\n",
    "# test_inp = pd.DataFrame(X_test_imp, columns = X_test.columns)\n",
    "\n",
    "# data_test = test_inp.as_matrix()\n",
    "# actual_Y = Y_actuals.as_matrix()\n",
    "\n",
    "# #test_inp.info()\n",
    "# actual_Y\n",
    "\n",
    "#test = np.c_[np.ones((len(test_inp)),1),data_test]\n",
    "#test = np.c_[np.ones((len(test_inp),1)),data_test]\n",
    "#test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicated labels for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 98170.29543207, 168708.04170487, 223012.8749381 , ...,\n",
       "       444691.3182825 , 166435.90459402, 185545.08820204])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_Y = testing_data.dot(theta.T)\n",
    "test_predicted_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_housing = data.drop(['median_house_value', 'ocean_proximity'],axis =1)\n",
    "# Y_housing = data[\"median_house_value\"].copy()\n",
    "\n",
    "# imputer.fit(X_housing)\n",
    "\n",
    "# X_housing_imp = imputer.transform(X_housing)\n",
    "# X_housing_inp = pd.DataFrame(X_housing_imp, columns = X_housing.columns)\n",
    "\n",
    "# X_housing_data = X_housing_inp.as_matrix()\n",
    "# housing_Y = Y_housing.as_matrix()\n",
    "# housing = np.c_[np.ones((len(X_housing_inp),1)),X_housing_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the Complete Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data , housing_labels = preprocessData(data)\n",
    "n_new = housing_data.shape[1]\n",
    "I = np.identity(n_new)\n",
    "I[0,0] = 0\n",
    "#Matrix \n",
    "A_new = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([238252.92341126, -53822.57959311, -54412.58749094,  13890.41026217,\n",
       "       -13090.60576714,  43065.99923065, -43402.47060121,  18379.81171344,\n",
       "        75166.34517979, -14920.2688228 , -58140.53857764, -18352.46936135,\n",
       "       -23486.15567601, 114899.43244337,      0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calulating new Parameter Matrix.\n",
    "theta_new = np.linalg.inv(training_data.T.dot(training_data) + (alpha)*A_new).dot(training_data.T).dot(training_labels)\n",
    "\n",
    "theta_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicated labels for whole housing dataset without splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([420407.95962734, 429613.00912515, 389857.80890539, ...,\n",
       "        78552.67039523,  89674.73674787, 107124.91700724])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predicted_Y = housing_data.dot(theta_new.T)\n",
    "\n",
    "housing_predicted_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Traning Data: 68433.95200602397\n",
      "RMSE for Test Data: 75276.69896705075\n",
      "RMSE for Housing Data taken without splitting: 76958.73383605707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_mse = mean_squared_error(testing_labels, test_predicted_Y)\n",
    "train_mse = mean_squared_error(training_labels,train_predicted_Y)\n",
    "housing_mse = mean_squared_error(housing_labels, housing_predicted_Y)\n",
    "\n",
    "\n",
    "print \"RMSE for Traning Data: \"+ str(np.sqrt(train_mse))\n",
    "print \"RMSE for Test Data: \" + str(np.sqrt(test_mse))\n",
    "print \"RMSE for Housing Data taken without splitting: \"+ str(np.sqrt(housing_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
